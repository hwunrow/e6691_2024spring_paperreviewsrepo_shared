# Attention Fundamentals Presentation

## Student contributors

- Johan Sweldens (jws2215)
- Han Yong Wunrow (nhw2114)
- Flor Sanders (fps2116)

## Overview

### Deliverables

- [Google Slides](https://docs.google.com/presentation/d/1BYBrl97vI80U-ev5vOuPhOiUbdNGUhI78vQQFHBIT0U/edit?usp=sharing)

Proposed structure:

- Intro
- Setup what / why
  - LSTM/RNNs and their issues
  - Inspiration from attention in nature
  - Historical context
- Initial architecture of LSTMs + Attention
  - (Neural Machine Translation by Jointly Learning to Align and Translate) --> Introduction of the architecture
  - Some of the results
  - A reference implementation
- Where attention
  - Different attention mechanisms
  - Give two examples and explain their differences
  - Show numerical examples
- How attention is used in different applications from there
  - Discuss applications + architectures
  - Hint towards the: do we actually need RNNs?
    --> Attention is all we need as cliffhanger

### References

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Effective approaches to attention-based neural machine translation](https://arxiv.org/abs/1508.04025)
- [Attention Mechanisms in Neural Networks - Where it comes and where it goes](https://arxiv.org/abs/2204.13154)
- [Attention Mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1)
- [X-Former: In-Memory Acceleration of Transformers](https://ieeexplore.ieee.org/abstract/document/10155455)
