# Attention Fundamentals

## Student contributors

- Johan Sweldens (jws2215)
- Han Yong Wunrow (nhw2114)
- Flor Sanders (fps2116)

## Deliverables

- [Google Slides](https://docs.google.com/presentation/d/1BYBrl97vI80U-ev5vOuPhOiUbdNGUhI78vQQFHBIT0U/edit?usp=sharing)
- [Literature Study Notes](./Papers_Notes.md)
- [Notebook Exploring Attention Mechanisms](./attention_mechanisms.ipynb)
- [Neural Machine Translation with Attention Demo](./seq2seq_translation_tutorial.ipynb) - Adapted from [PyTorch Demo](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

## References

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Effective approaches to attention-based neural machine translation](https://arxiv.org/abs/1508.04025)
- [Attention Mechanisms in Neural Networks - Where it comes and where it goes](https://arxiv.org/abs/2204.13154)
- [Show, attend and tell: Neural image caption generation with visual attention](https://arxiv.org/abs/1502.03044)
- [Attention-Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503)
- [Attention Mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1)
- [Attention Mechanism - What are Query, Key and Value?](https://www.anthonychiu.xyz/blog/attention-mechanism)
- [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
