# Attention Fundamentals
<<<<<<< HEAD

## References

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Effective approaches to attention-based neural machine translation](https://arxiv.org/abs/1508.04025)
- [Attention Mechanisms in Neural Networks - Where it comes and where it goes](https://arxiv.org/abs/2204.13154)
- [Attention Mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1)
- [X-Former: In-Memory Acceleration of Transformers](https://ieeexplore.ieee.org/abstract/document/10155455)

## TODO student contributors
=======
## Student contributors
- Johan Sweldens (jws2215)
- Han Yong Wunrow (nhw2114) 
- Flor Sanders (fps2116)
>>>>>>> 6f28e39b0ce4eb1cbf6c658e331749aafb465d40

## TODO add content
